{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/vijay.rajan/Desktop/datasets/BYOC/lending_club.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding which columns have missing values\n",
    "\n",
    "have_missing_values = data.columns[data.isna().any()].tolist()\n",
    "have_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating columns with missing values for imputation\n",
    "\n",
    "features_with_missing_values = data[have_missing_values]\n",
    "\n",
    "# Select columns with categorical values so I can do encoding on those\n",
    "\n",
    "categorical_features = data.select_dtypes('object')\n",
    "\n",
    "# Separating rest of the data so I can later merge it with the preprocessed features\n",
    "\n",
    "no_preprocessing_needed = data.drop(columns=(list(categorical_features.columns) + list(features_with_missing_values.columns)), \n",
    "                                    axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing meadian for all the missing values and creating a missing value feature flag\n",
    "\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "\n",
    "indicator = MissingIndicator(missing_values=np.nan)\n",
    "mask_missing_values_only = indicator.fit_transform(features_with_missing_values)\n",
    "\n",
    "# Creating feature flags and naming them with the prefix 'missing_flag_'\n",
    "\n",
    "mask_missing_values_only = pd.DataFrame(mask_missing_values_only, columns=['missing_flag_' + name for name in list(features_with_missing_values.columns)])\n",
    "\n",
    "# Imputing missing values with median of the column\n",
    "imp = SimpleImputer(missing_values = np.nan, strategy = 'median')\n",
    "imp.fit(features_with_missing_values)\n",
    "SimpleImputer()\n",
    "\n",
    "# Creating a dataframe with the columns where missing values imputation was done\n",
    "\n",
    "features_with_imputed_values = pd.DataFrame(imp.transform(features_with_missing_values), columns=list(features_with_missing_values.columns))\n",
    "\n",
    "after_imputation = pd.concat([mask_missing_values_only, features_with_imputed_values], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding for the categorical features\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded_features = [one_hot.fit_transform(categorical_features[[name]]).toarray() for name in list(categorical_features.columns)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the list of names for the encoded columns and creating a dataframe of encoded features\n",
    "\n",
    "encoded_column_names = []\n",
    "for col_name in list(categorical_features):\n",
    "    for i in range(0, data[col_name].nunique()):\n",
    "        encoded_column_names.append(col_name + '_' + str(i))\n",
    "        \n",
    "encoded_df = pd.concat([pd.DataFrame(element) for element in encoded_features], axis='columns')\n",
    "encoded_df.columns = encoded_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the final dataframe\n",
    "\n",
    "final_df = pd.concat([no_preprocessing_needed, after_imputation, encoded_df], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the train-test split\n",
    "\n",
    "y = final_df.loc[:, 'is_bad']\n",
    "X = final_df.iloc[:, 1:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n",
    "\n",
    "# Scaling all the features except for the target is_bad\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard_scalar = StandardScaler()\n",
    "X_train = standard_scalar.fit_transform(X_train)\n",
    "X_test = standard_scalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred, labels=[0, 1]))\n",
    "metrics.roc_auc_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
